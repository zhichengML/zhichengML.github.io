---
layout: post
title: 11.How can Machine Learn Better? - Overfitting and Solution
categories: [ReadNote]
tags: ReadNote-Machine-Learning-Foundation
---

# How can Machine Learn Better? - Overfitting and Solution
----------------------------------
<!-- TOC depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 -->

- [How can Machine Learn Better? - Overfitting and Solution](#how-can-machine-learn-better-overfitting-and-solution)
	- [1. What is Overfitting?](#1-what-is-overfitting)
	- [2. Dealing with Overfitting](#2-dealing-with-overfitting)
		- [1) Start from Simple Model](#1-start-from-simple-model)
		- [2) Data Cleaning/Pruning](#2-data-cleaningpruning)
		- [3) Data Hinting](#3-data-hinting)
		- [4) Regularization](#4-regularization)
		- [5) Validation](#5-validation)
	- [Summary](#summary)
	- [Reference](#reference)

<!-- /TOC -->

</br></br>


## 1. What is Overfitting?
上一节最后，我们提到了如果线性模型的模型复杂度太大的话，可能会引起Overfitting。同样的很明显会有Underfitting的情况。
> 那什么是Overfitting，Underfitting呢？

首先根据名字，Overfitting：over fitting，就是在fitting的时候太over了。我们用线性模型去分类/回归处理数据的过程就是一个fitting的过程，所以也就是说我们处理过头了。同理Underfitting就是处理不够到位。

> 那么什么时候才是处理过头呢？什么时候才是处理不到位呢？

就是在处理相对简单的问题的时候用了相对复杂的模型去处理。
就是在处理相对复杂的问题的时候用了相对简单的模型去处理。

我们用下面的例子来进行说明
1.首先例子如图一所示(这里用的是Ng的图，因为在林老师的ppt中没找到很好地图同时体现Underfit, good fit overfit)。左图是欠拟合(underfit)，中间的图四好的拟合（good fit），右图是过度拟合（overfit）。单纯从拟合结果来看：明显左边和中间的图 $E_{in}$ 比右图要大。但是从泛化好坏来看，显然左图和中间的图要比右图好。
假如我们考虑good fit分类出错的点为噪音点（noise），那么Overfit的模型就会受到了严重的干扰。

![Underfit, Good Fit and Overfit](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/faeeb6ed9dd42d75c5b4b20d6b9a592c92ce7ece/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter11-3%20Underfit%20Good%20Fit%20and%20Overfit.png)
<center> 图一 Underfit, Good Fit and Overfit <sup>[1]</sup></center>
</br>


2. 接着我们回头看之前总结的VC Dimension 的曲线， 如图二所示。

![Learning Curve](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/17974844acd6bf87d8cf4d68731f9d4cade5b450/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter11-2%20Learning%20Curve%20.png)

图二 Learning Curve <sup>[1]</sup>
</br>

图中可以看到在VC Dimension变大时，$E_{in}$ 变小， 但 $E_{out}$ 先变小后变大，而过拟合和欠拟合的情况的主要区别就在于 $E_{out}$ 的变化情况，具体解释如下：
- 过拟合（Overfitting）发生在VC Dimension较大时，$E_{in}$ 太小， 但 $E_{out}$ 太大（即VC Dimension 太大了） ，表示在训练样本上拟合做的很好，$E_{in}$ 太小，但是过度了，使得泛化能力变差， $E_{out}$ 很大
- 欠拟合（Underfitting）发生在在VC Dimension较小时，$E_{in}$ 太大，同时 $E_{out}$ 太大（即VC Dimension 太小了），表示在训练样本上拟合做不够好，$E_{in}$ 太大，虽然泛化能力很强（即 $E_{out}$ 也太大）

关于如何解决欠拟合的问题之前也讨论过： 从低到高不断地提高多项式次数，使得VC维提高，达到拟合的效果。
但过拟合的问题更为复杂，下面会更深入的探讨。


3. 下面的图三，可以让我们更加直观的看到overfitting造成的问题

![Cases of Overfitting](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/9cbdf35a9838985e1f889100ce389c4d316cc0f0/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter11-4%20Case%20of%20Overfitting.png)
<center> 图三 Case of Overfitting <sup>[2]</sup></center>
</br>

从图中可以看到，Overfitting的 $E_{in}$ 都比 good-fit的要低，但是 $E_{out}$ 却很高（泛化能力差）。


总结起来有3个因数会导致Overfitting的发生：
- data size N 太小
- noise 太多
- VC Dimension太大

</br></br>
----------------------------------


## 2. Dealing with Overfitting
上一节我们提出了overfitting并作了分析。总结出3个因数会导致overfitting，下面根据这3个因数，我们有5种方法帮助我们避免overfitting的发生。

- 使用简单的模型(start from simple model)，逐次增加模型的复杂度 - 防止 VC Dimension太大
- 进行数据清理/裁剪(data cleaning/pruning) - 防止 noise 太多
- 数据提示（data hinting） - 防止 data size N 太小
- 正则化（regularization） - 防止 VC Dimension太大
- 确认（validation） - 提取一部分的数据作为测试集，提前估计模型的泛化强度


下面我们分别介绍这5种方法，其中前三种方法比较简单，这里不做深入讨论，而 Regularization 和 Validation 较复杂，这里会用比较多的笔墨进行讨论。

### 1) Start from Simple Model
上一章中也提到过，由于VC Dimension太大的话，导致 $E_{in}$ 变小的同时 $E_{out}$却在变大。所以我们如果从d=1阶的模型开始debug，如果 $E_{in}$ 不符合要求，那么我们增大d为2阶，然后在进行debug，以此类推，直到 $E_{in}$ 符合我们要求位置，这个时候的 VC Dimension 不会很大，而且我们也得到泛化能力相对较强的模型。


### 2) Data Cleaning/Pruning
Data cleaning/pruning就是对训练数据集里label有明显错误的样本进行清理（data cleaning）或者裁剪（pruning)。data cleaning/pruning关键在于如何准确寻找label错误的点或者是noise的点。而处理的方法为
- 纠正，即数据清理（data cleaning）的方式处理该情况；
- 删除错误样本，即数据裁剪（data pruning）的方式处理。

处理措施很简单，但是发现样本是噪音或离群点却比较困难。


### 3) Data Hinting
Data hinting是针对N不够大的情况，通过data hinting的方法就可以对已知的样本进行简单的处理、变换，从而获得更多的样本。比如说：数字分类问题，可以对已知的数字图片进行轻微的平移或者旋转，从而得到更多的数据，达到扩大训练集的目的。这种通过data hinting得到的数据叫做：virtual examples。

> 需要注意的是，新获取的virtual examples可能不再是iid某个distribution。所以新构建的virtual examples要尽量合理，且是独立同分布的。

### 4) Regularization
Regularization（正规化）处理属于penalized方法的一种，通过正规化的处理来对原来的方程加上一个regularizer进行penalize，从而使得过渡复杂的模型，变得没那么复杂。

关于Regularization 的讨论看此链接:
[12. 机器学习基石 - Regularization](blog.csdn.net/jasonwoolf/article/details/78222996)

### 5) Validation
这个是目前最常用的方法之一，通过提前把一部分的数据拿出来作为测试集，因为测试集是随机取出来的，而将来实际的应用中，数据也大体和测试集出入不大，所以用这种方法，可以提前得到实际应用的时候，模型的错误 $E_{out}$ 通过这个作为衡量模型是否合格的条件之一。

关于Validation 的讨论看此链接
[13. 机器学习基石 - Validation](http://blog.csdn.net/jasonwoolf/article/details/78223014)




</br></br>
----------------------------------

## Summary
1.首先介绍了Overfitting和Underfitting的概念。

2.接着我们着重分析Overfitting，总结了产生Overfitting的原因：

- data size N 太小

- noise 太多

- VC Dimension太大

3.最后我们分析如何最大程度的避免Overfitting。在solution中.



</br></br>
----------------------------------

## Reference
[1] 机器学习基石(台湾大学-林轩田)\13\13 - 1 - What is Overfitting- (10-45)

[2] 机器学习基石(台湾大学-林轩田)\13\13 - 2 - The Role of Noise and Data Size (13-36)
