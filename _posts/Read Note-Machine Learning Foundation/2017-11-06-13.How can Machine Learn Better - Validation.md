# How can Machine Learn Better? - Validation
----------------------------------
<!-- TOC depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 -->

- [How can Machine Learn Better? - Validation](#how-can-machine-learn-better-validation)
	- [1. Model Selection Problem](#1-model-selection-problem)
	- [2. Validation](#2-validation)
	- [3. Leave-One-Out Cross Validation](#3-leave-one-out-cross-validation)
	- [4. V-Fold Cross Validation](#4-v-fold-cross-validation)
	- [Summary](#summary)
	- [Reference](#reference)

<!-- /TOC -->

</br></br>

> 这一节我们Validation，需要选择的原因是机器学习的算法有很多种，如何评估哪一种适合当前的使用场景是一个值得商榷的问题。所以通过Validation，我们可以对模型的误差有一定的测试，评估，从而可以确定我们所需要的模型。

## 1. Model Selection Problem
到目前为止，已经学过了许多算法模型，但一个模型需要很多参数的选择，这是本章讨论的重点。

以二元分类为例，在学过的算法有PLA、pocket、线性回归、logistic回归；算法中需要用到迭代方法，就需要选择迭代步骤T；同理也需要选择学习步长 ；处理非线性问题时，需要用到不同的转换函数，可能是线性的，二次式，10次多项式或者10次勒让德多项式；如果加上正则化项，则可以选择L2正则化项，L1正则化项；有了正则化项，则参数 值也需要选择。

在如此多的选项中设计一个适用于各种情况各个数据集的模型显然是不可能的，因此针对不同的情况，对各个参数的选择是必须的。

我们主要是通过误差 E 来进行判断一个模型是否良好的。我们有 $E_{out}$ 和 $E_{in}$
1. 显然我们不能通过 $E_{out}$ 来进行评估，因为我们不可能拿得到未来要进行测试用的数据。
2. 但是我们也不能通过 $E_{in}$ 来进行并评估，因为从前面2节的讨论中，我们知道，模型复杂度越高的模型， $E_{in}$ 往往就越高，但是也造成了模型的泛化能力变弱，使得  $E_{out} \approx E_{in}$ 的条件不成立，根本谈不上机器学习了。

但是我们能否采用另一个相对宽松的误差来作为评估呢？比如说:有这样一个独立于训练样本的测试集，将M个模型在测试集上进行测试，看一下Etest的大小，则选取Etest最小的模型作为最佳模型。
> 是可以的，因为我们有Hoeffding Inequity作为理论保证

下面我们将讨论这个宽松的误差。
1.首先，这个最小的测试集选出来的权值满足公式（1），其中 $m^\ast$ 表示最佳的权值

$$
\DeclareMathOperator*{\argmin}{argmin}
m^* = \argmin\limits_{1 \leq m \leq M} (E_m = E_{test}(A_m(D)))
\tag{$1$}
$$

2.这种测试集验证的方法，根据finite-bin Hoffding不等式，可以得到公式（2），并且可以看出，模型个数M越少，测试集数目越大，那么 $O(\sqrt{\frac{logM}{N_{test}}}$ 越小，即 $E_{test}(g_{m^\ast})$ 越接近于 $E_{out}(g_{m^\ast})$ 。

$$
E_{out}(g_{m^\ast}) \leq E_{test}(g_{m^\ast}) + O(\sqrt{\frac{logM}{N_{test}}}
\tag{$2$}
$$



下面比较一下使用 $E_{in}$ 和 $E_{test}$ 作为评估基准的方法：
1. 使用 $E_{in}$ 作为判断基准，使用的数据集就是训练集D本身；不仅使用D来训练不同的 $g_{m^\ast}$ ，而且又使用D来选择最好的 $g_{m^\ast}$ ，那么 $g_{m^\ast}$ 对未知数据并不一定泛化能力好。课堂中举的例子是：，老师用学生做过的练习题训练学生，然后再用一样的题目来对学生进行考试，这种方法即使学生得到高分，也不能说明他的真的掌握了知识。所以最小化 $E_{in}$ 的方法并不科学。
2. 第使用 $E_{test}$ 作为判断基准，使用的是独立于训练集D之外的测试集。而后者使用的是独立于D的测试集，相当于用一份题目来训练学生后，再用另外一份全新的题目来测试学生的水平，这样更能反映出学生对知识点的理解，所以最小化 $E_{test}$ 更加理想。但是真正的训练集是拿不到的（要实际投入使用才知道）。所以，寻找一种折中的办法，我们可以使用已有的训练集D来创造一个验证集validation set，即从D中划出一部分 $D_{val}$ 作为验证集。D另外的部分作为训练模型使用， $D_{val}$ 独立开来，用来测试各个模型的好坏，最小化 $E_{val}$ ，从而选择最佳的 $g_{m^\ast}$ ∗。

>下面，我们针对验证集的选择进行讨论。


</br></br>
----------------------------------
## 2. Validation
验证集的选择，我们必须遵循几点：
1. 选择要随机 - 造成数据不平衡，结果有问题
2. 数据量分配要合理 - 过多的验证集会造成训练集的减少，过少的验证集验证结果可信度不高
3. 验证集必须是新的，没有被用于训练 - 那就等于是直接用训练集去做验证了

所以为了满足这3个条件，我们下面需要讨论如何来划分数据集

1.首先我们将原本的数据样本D分为两部分： 训练数据 $D_{train}$ 和 验证数据 $D_{val}$， 如公式（3）所示，如图一所示。

$$
\underbrace{D}_{size=N} = \underbrace{D_{train}}_{size=N-K} \cup \underbrace{D_{val}}_{size=K}
\tag{$3$}
$$

![Validation Set](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/9bf9752d63dd11988ef0024464fe257c99f4827a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter13-1%20Validation%20Set.png)
<center> 图一 Validation Set <sup>[1]</sup></center>
</br>


2.根据Hoeffding Inequity，我们可以得到公式（4）

$$
E_{out}(g_{m*}) \leq E_{out}(g_{m^-}) \leq E_{test}(g_{m^-}) + O(\sqrt{\frac{logM}{K}}
\tag{$2$}
$$

下面我们举个例子来解释这种模型选择的方法的优越性，假设有两个模型：一个是5阶多项式HΦ5，一个是10阶多项式HΦ10。通过不使用验证集和使用验证集两种方法对模型选择结果进行比较，分析结果如图二所示

![Validation in Practice](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/70d9d9faae5aed3273361b2fb4a776cf83c9fb27/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter13-2%20Validation%20in%20Practice.png)
<center> 图二 Validation in Practice <sup>[1]</sup></center>
</br>

图中，横坐标表示验证集数量K，纵坐标表示 $E_{out}$ 大小。黑色水平线表示没有验证集，完全使用 $E_{in}$ 作为判断基准，那么 $H_{\Phi 10}$ 更好一些，但是这种方法的 $E_{out}$ 比较大，而且与K无关。黑色虚线表示测试集非常接近实际数据，这是一种理想的情况，其 $E_{out}$ 很小，同样也与K无关，实际中很难得到这条虚线。

红色曲线表示使用验证集，但是最终选取的矩是 $g_{m^\ast}^-$ ，其趋势是随着K的增加，它对应的 $E_{out}$ 先减小再增大，当K大于一定值的时候，甚至会超过黑色水平线。蓝色曲线表示也使用验证集，最终选取的矩是 $g_{m^\ast}$ ，其趋势是随着K的增加，它对应的 $E_{out}$ 先缓慢减小再缓慢增大，且一直位于红色曲线和黑色直线之下。从此可见，蓝色曲线对应的方法最好，符合我们之前讨论的使用验证集进行模型选择效果最好。

但是也存在问题，当 K 太大的时候，红色曲线会超过黑色直线，也就是上面提到的，选择了太多的数据作为验证集，从而导致训练的数据大大减少，那么模型的泛化能力太差，不能保证 $E_{in} \approx E_{out}$

>那么如何选择 K呢？

老师的建议是选择 $K = \frac{N}{5}$




</br></br>
----------------------------------
## 3. Leave-One-Out Cross Validation
上一节，最后我们提到了K值得重要性，下面我们将讨论如何让这个数据取得更加合理。

第一种方法叫做： Leave-One-Out Cross Validation(留一法交叉验证)，这方法是采用一种极端的情况（K=1），也就是说验证集大小为1，即每次只用一组数据对 $g_m$ 进行验证，重复N次，最后求平均误差。

这种算法的优缺点如下：
- 优点：使得 $g_m^- \approx g_m$
- 缺点：①$E_{val}$ 与 $E_{out}$  可能会相差很大 ②时间和空间复杂度非常大 ③稳定性差（例如对于二分类问题，取值只有0和1两种，预测本身存在不稳定的因素，那么对所有的Eloocv计算平均值可能会带来很大的数值跳动）

误差方程的表达式如公式（3）所示。

$$
E_{loocv}(H, A) = \frac{1}{N} \sum\limits_{n=1}^{N} \cdot err(g_n^-(x_n), y_n)
\tag{$3$}
$$


该算法的具体流程如下：

1.取n=1的点出来做验证集，其他点保留作为训练集，求出Hypothesis 和对应的 误差 $err(g_n^-(x_n), y_n)$

2.接着取n=2的点出来做验证集，其他点保留作为训练集，求出Hypothesis 和对应的 误差 $err(g_n^-(x_n), y_n)$

3.以此类推，重复N次，直到n=N的点也进行上述的操作

4.把N次误差加起来求平均

课上举了一个例子说明该方法的效果并且总结了Feature的多少与 $E_{in} E_{out} E_{loocv}$的关系。如图三所示。很明显可以看出，使用Ein发生了过拟合，而Eloocv分类效果更好，泛化能力强。


![Leave-One-Out in Practice](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a5de867bdf6c5738844287c3de950431325891b0/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter13-4%20Leave-One-Out%20in%20Practice.png)
<center> 图三 Leave-One-Out in Practice <sup>[2]</sup></center>
</br>


但是这种方法由于他的缺点太多，所以实际中往往很少使用。更多的是使用下面要讨论的V-Fold Cross Validation


</br></br>
----------------------------------
## 4. V-Fold Cross Validation
上面讨论的Leave-One-Out Cross Validation 的优缺点导致了该算法不好用。所以针对该算法的缺点，人们对其改进为V-Fold Cross Validation: 这种算法是把数据N平均分为V分，每次取一份作为验证集，剩下的V-1份作为测试集，重复V次，算出平均误差。那样的话Leave-One-Out Cross Validation 可以看成是V=N的极端情况

该算法的误差方程如公式（4）所示

$$
E_{cv}(H, A) = \frac{1}{V} \sum\limits_{V=1}^{V} E_{val}^{(v)}(g_v^-)
\tag{$4$}
$$

一般的Validation使用V-Fold Cross Validation来选择最佳的模型。

但是该算法也有一点问题：就是Validation的数据来源是集中的，所以并不能保证交叉验证的效果好，得到的数学模型一定就很好。只有在样本数据足够多的时候，结果才可信，数学模型泛化能力越强。


</br></br>
----------------------------------

## Summary
1. 首先介绍了validation验证的意义
2. 然后介绍了Leave-One-Out算法并分析其优劣：该算法在实际应用不多
3. 最后根据Leave-One-Out算法的缺点，做改进，提出了V-Fold Cross Validation

</br></br>
----------------------------------

## Reference
[1] 机器学习基石(台湾大学-林轩田)\15\15 - 1 - Model Selection Problem (16-00)

[2] 机器学习基石(台湾大学-林轩田)\15\15 - 3 - Leave-One-Out Cross Validation (16-06)
