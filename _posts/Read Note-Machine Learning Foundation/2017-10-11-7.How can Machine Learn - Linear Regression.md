---
layout: post
title: 7.How can Machine Learn? - Linear Regression
categories: [ReadNote]
tags: ReadNote-Machine-Learning-Foundation
---

# How can Machine Learn? - Linear Regression
----------------------------------

<!-- TOC depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 -->

- [How can Machine Learn? - Linear Regression](#how-can-machine-learn-linear-regression)
	- [1. What is Regression](#1-what-is-regression)
	- [2. Linear Regression](#2-linear-regression)
		- [1) Introduction of Linear Regression](#1-introduction-of-linear-regression)
		- [2) Error Measurement of Linear Regression](#2-error-measurement-of-linear-regression)
		- [3) Linear Regression Algorithm](#3-linear-regression-algorithm)
		- [4) Generalization Issue](#4-generalization-issue)
		- [5) Linear Regression for Binary Classification](#5-linear-regression-for-binary-classification)
	- [Summary](#Summary)
	- [Reference](#reference)

<!-- /TOC -->

</br></br>
----------------------------------

## 1. What is Regression


机器学习算法一般是这样一个步骤：

1. 对于一个问题，用数学语言来描述它，然后建立模型，e.g., 回归模型或者分类模型；

2. 建立代价函数: Cost Function, 用最大似然、最大后验概率或者最小化分类误差等等数，也就是一个最优化问题。找到最优化问题的解，也就是能拟合我们的数据的最好的模型参数；

3. 求解这个代价函数，找到最优解。求解也就分很多种情况了：

   1). 如果这个优化函数存在解析解。例如我们求最值一般是对代价函数求导，找到导数为0的点，也就是最大值或者最小值的地方了。如果代价函数能简单求导，并且求导后为0的式子存在解析解，那么我们就可以直接得到最优的参数了: Gradient Descent。

   2). 如果式子很难求导，例如函数里面存在隐含的变量或者变量相互间存在耦合，也就互相依赖的情况。或者求导后式子得不到解释解，例如未知参数的个数大于已知方程组的个数等。这时候我们就需要借助迭代算法来一步一步找到最有解了。

   3). 另外需要考虑的情况是，如果代价函数是凸函数，那么就存在全局最优解。


回归(Regression)问题与分类(Classification)问题类似，关键的区别在于，Regression的输出是实数,是一个范围，所以不能提前确定所有输出值；而Classification的输出是确定的值。换个角度说，假如输出足够多的数据的时候：如果输出的值是连续的，这个问题叫做Regression，而如果输出的值是离散的，那么这个问题叫做Classification


用数学符号表示如果公式（1）所示。

$$
\begin{align*}
Regression : &f(x) = \left[ value1, value2 \right]  \\
Classification: &f(x) = \{ value_1, value_2, ..., value_n \}
\end{align*}
\tag{$1$}
$$

</br></br>
----------------------------------

## 2. Linear Regression
### 1) Introduction of Linear Regression
线性回归问题同样的表示各个属性（Attribution）对最终结果的贡献：也就是说每个属性乘以对应的权值，最后再加上一定的偏移，如公式（2）所示

$$
y = w_0 + w_1 \cdot x_1 + w_2 \cdot x_2 + ... + w_n \cdot x_n
\tag{$2$}
$$

如果给第一项的偏移 $w_0$ 加上属性值 $x_0$，并把单独的输出点$y$用向量进行表示，那么公式（2）就变成了公式（3）（这一步纯粹是为了表示方便）。

Hypothesis h(x):
$$
\begin{align*}
h(x) &= w_0 \cdot x_0 + w_1 \cdot x_1 + w_2 \cdot x_2 + ... + w_n \cdot x_n  \quad(x_0 = 1) \\
     &= w^T \cdot x_i \\
     &= W^T \cdot X
\end{align*}
\tag{$3$}
$$
从公式(3)的可以看出，与二元分类假设函数的表示只差了一个取正负号的函数 $sign()$。


### 2) Error Measurement of Linear Regression
线性回归问题的错误如图一所示。
![Error Measurement](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/de4c8798497f08b40cbe86cf0cc665ceaa6d54ff/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-1%20Linear%20Regression.png)
<center> 图一 Error Measurement <sup>[1]</sup></center>
</br>

上一章中，提到了回归问题我们用平方误差来表示（其实还有RMSE， R2等方法，老师上课只讲了平方误差，为了说明方便，我们只写这个）
根据图一的信息，很明显得到公式（4）（5）。

$$
E_{in}(w)) = \frac{1}{N} \sum\limits_{n=1}^{N}(w^Tx_n - y_n)^2
\tag{$4$}
$$

$$
E_{out}(w)) = \epsilon \cdot \sum\limits_{n=1}^{N}(w^Tx_n - y_n)^2
\tag{$5$}
$$
VC Bound可以约束各种情况的学习模型，当然回归类型的模型也被也受此约束，只需要寻找足够小 $E_{in}(w)$ 便可以满足 $E_{out}(w)$ 小的需求。



### 3) Linear Regression Algorithm
刚刚上面的Error Meansurement中，我们提到了VC Bound中，在训练样本足够的情况下，我们要用特定的算法，来找到最小的 $E_{in}(w)$，所以把上面公式（4）的pointwise error measurement 通过矩阵做调整得到公式（6）。

$$
\begin{align*}
E_{in}(w)) &= \frac{1}{N} \sum\limits_{n=1}^{N}(w^Tx_n - y_n)^2 \\
           &= \frac{1}{N} \sum\limits_{n=1}^{N}(x_n^Tw - y_n)^2   \quad\quad (向量内积，符合交换率) \\
           &= \frac{1}{N}
           \left|
           \left|
           \begin{array}{ccc}
                x_1^Tw - y_1 & \\
                x_2^Tw - y_2 & \\
                ...            \\
                x_N^Tw - y_N &
           \end{array}
           \right|
           \right|^2               \quad\quad(平方求和)\\
           &= \frac{1}{N}
           \left|
           \left|
           \begin{bmatrix}
                -x_1^T-\\
                -x_1^T-\\
                ...    \\
                -x_1^T-\\
           \end{bmatrix}
           \cdot w
           \begin{bmatrix}
                y_1\\
                y_2\\
                ...\\
                y_n\\
           \end{bmatrix}
           \right|
           \right|^2                \quad\quad(矩阵平方)\\

           &= \frac{1}{N}
           \left|
           \left|
           \begin{array}{ccc}
                \underbrace{X^T}_{N \times (d+1)}
                \underbrace{w}_{(d+1)\times 1}
                -
                \underbrace{y}_{N \times 1}
           \end{array}
           \right|
           \right|^2 \\
\end{align*}
\tag{$6$}
$$

根据公式（6），那么 $E_{in}(w)$的最小值如公式（7）所示。

$$
min_w E_{in}(w) = min_w \frac{1}{N} \left|\left| X^Tw - y \right|\right|^2
\tag{$7$}
$$

$E_{in}(w)$的变化图如图二所示，可以看出这是一个连续（continuous）、可微（differentiable）的凸（convex）函数。最小值就是山谷的点（即转折点），并且对这个点求导的值为0。
![Ein Curve](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/edb6705ac1e3e9565cb69b9b170c1de25f86f987/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-2%20Linear%20Regression%20Ein.png)
<center> 图二 Ein Curve <sup>[1]</sup></center>

所以接下来，我们需要对$E_{in}(w)$求导并且找到一个值 $W_{LIN}$，使得 $\nabla E_{in}(w)$ 的值为0。
1. 首先我们需要考虑两种情况：①只有一个w； ②w为向量
2. 首先我们把公式（6）平方去掉
3. 只有一个w的时候，就按照正常的求导方式求导即可，得到公式（7）
4. 当w为向量的时候，我们需要对其中一个进行转置，然后再求导，得到公式（8）
5. 根据公式（7）（8）可以看出，两种情况的求导的结果非常相似，可以用公式（9）表示
6. 最后我们令公式（9）的值为0，得到我们需要的 $W_{LIN}$ 如公式（10）所示,其中 $X^\dagger$ 叫做[矩阵X的伪逆矩阵（pseudo-inverse）](http://blog.sina.com.cn/s/blog_890c6aa30101cn6t.html)

$$
\begin{align*}
one \quad w : E_{in}(w) &= \frac{1}{N} \left( (X^T)X w^2 - 2 \cdot X^T yw  + y^2 \right) \\
       \nabla E_{in}(w) &= \frac{1}{N} \left( 2(X^T)X w - 2 \cdot X^T y \right)
\end{align*}
\tag{$7$}
$$

$$
\begin{align*}
vector \quad w :  E_{in}(w) &= \frac{1}{N} \left( (X^T)X w^T w - 2 \cdot X^T y w^T  + y^2 \right) \\
          \nabla E_{in}(w)  &= \frac{1}{N} \left( 2(X^T)X w - 2 \cdot X^T y \right)
\end{align*}
\tag{$8$}
$$

$$
\nabla E_{in}(w)  = \frac{2}{N} \left( (X^T)X w - X^T y \right)
\tag{$9$}
$$

$$
\begin{align*}
W_{LIN} &= \underbrace{(X^T X) ^ {-1} X^T }_{X^\dagger} y \\
        &= \underbrace{X ^\dagger }_{(d+1)\times N)}
           \underbrace{y}_{N \times 1}
\end{align*}
\tag{$10$}
$$

### 4) Generalization Issue
上一节中，我们得到了最佳的 w 的解，但是，哪个是最小的 $E_{in}$， 实际上我们最需要的是使得 $E_{out}$最小，而我们这一节就是要讨论之前关于VC Bound保证了 $E_{in} \approx E_{out}$ 是否依然适用于线性回归问题。（中间解释过程较复杂，后续补充。）

结论就是得到了 $E_{in} E_{out}$关于 noise level的方程，如公式（11）（12），曲线图如图三所示。

$$
E_{in}(w)  = noise \quad level  \cdot (1- \frac{d+1}{N})
\tag{$11$}
$$

$$
E_{out}(w)  = noise \quad level  \cdot (1+ \frac{d+1}{N})
\tag{$12$}
$$

![Ein Eout Curve](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/01235bfa1a94084fee01577e205a27839be75228/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-3%20Ein%20Eout.png)
<center> 图三 Ein Eout Curve<sup>[2]</sup></center>
</br>

可以看出在N趋于无穷大时，与两者都会趋近于noise level的值，即 $\sigma^2$ 泛化错误之间的差距：$\frac{2(d+1)}{N}$ 。

至此可以表明在线性回归中可以寻找到很好的 $E_{out}$，因此线性回归可以学习。





### 5) Linear Regression for Binary Classification
这一节，我们主要讨论能否通过求解线性回归的方式求二元分类问题

首先对比Linear Classification 和 Linear Regression， 如图四

![Linear Classification and Linear Regression](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/d4a67d82122ef663fb09d41c3b1862d897536e2a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-4%20Linear%20Regression%20for%20Binary%20Classification.png)
<center> 图四 Linear Classification and Linear Regression<sup>[3]</sup></center>
</br>

从求解问题的难度考虑，二元分类的求解是一个NP难问题，只能近似求解，而线性回归求解方便，程序编写也简单。

直觉告诉我们:因为二元分类的输出空间{-1，+1}属于线性回归的输出空间，即 $\{-1, +1\}∈ R$。其中数据集的标记大于零的表示+1，小于零的表示-1，通过线性回归求得的解析解 $W_{LIN}$，直接得出最优假设 $g(x) = sign( W_{LIN} x)$ 。

下面给出证明：
> 入手点：误差大小

首先对比两种方法的曲线图，如图五所示。
![Relations of Two Errors](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/341baa79de829072a35424870d65421a0bc9f9ba/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-5%20Relation%20of%20Two%20Errors.png)
<center> 图五 Relations of Two Errors<sup>[3]</sup></center>
</br>

可以发现，无论期望的y为-1还是+1， $err_{sqr}$ 始终大于 $err_{0/1}$，即公式（13），加上我们之前有的 $E_{out} \approx E_{in}$，那么我们可以得到公式（14）。


$$
err_{0/1} \leq err_{sqr}
\tag{$13$}
$$


$$
\begin{align*}
Classification \quad E_{out}(w)
&\leq Classification \quad E_{in}(w)  + \sqrt{\frac{8}{N} \ln\left( \frac{4 (2N)^{d_{vc}}}{\delta} \right)} \\
&\leq Regression \quad\quad E_{in}(w)  + \sqrt{\frac{8}{N} \ln\left( \frac{4 (2N)^{d_{vc}}}{\delta} \right)}
\end{align*}
\tag{$14$}
$$


所以我们可以通过使用 Linear Regression 的方法来求解Linear Classification的问题，虽然通过这种方法，得到的误差可能更大，但却是一种更有效率的求解方式。在实际运用中，一般都将通过线性回归求得的解析解作为PLA或者pocket的初始值，达到快速求解的目的 。
</br></br>
----------------------------------


## Summary
1. 首先介绍了Regression。
2. 然后通过例子引入Linear Regression，分别介绍Linear Regression的误差方程，算法流程。
3. 接着我们引入VC Bound 解释了Linear Regression为什么可以实现学习。
4. 最后我们应用Linear Regression的算法到Classification的问题中：虽然会损失一定的准确度，但是效率提升很大。

</br></br>
----------------------------------

## Reference
[1] 机器学习基石(台湾大学-林轩田)\9\9 - 1 - Linear Regression Problem (10-08)

[2] 机器学习基石(台湾大学-林轩田)\9\9 - 3 - Generalization Issue (20-34)

[3] 机器学习基石(台湾大学-林轩田)\9\9 - 4 - Linear Regression for Binary Classification (11-23)
