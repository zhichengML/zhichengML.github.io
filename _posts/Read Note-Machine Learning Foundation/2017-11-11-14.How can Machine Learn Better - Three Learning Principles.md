---
layout: post
title: 14.How can Machine Learn Better? - Three Learning Principles
categories: [ReadNote]
tags: ReadNote-Machine-Learning-Foundation
---

# How can Machine Learn Better? - Three Learning Principles
----------------------------------
<!-- TOC depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 -->

- [How can Machine Learn Better? - Three Learning Principles](#how-can-machine-learn-better-three-learning-principles)
	- [1. Occam’s Razor](#1-occams-razor)
	- [2. Sampling Bias](#2-sampling-bias)
	- [3. Data Snooping](#3-data-snooping)
	- [Summary](#summary)
	- [Reference](#reference)

<!-- /TOC -->

</br></br>

> 这节课， 主要是介绍提高机器学习性能三个实用方法 Occam’s Razor, Sampling Bias, Data Snooping。

## 1. Occam’s Razor
奥卡姆剃刀定律（Occam’s Razor），这个原理称为“如无必要，勿增实体”（Entities must not be multiplied unnecessarily），就像剃刀一样，将不必要的部分去除掉。将奥卡姆剃刀定律应用在机器学习上意思是使用的模型尽可能的简单。

如下面图一的例子

![Occam's Razor for Learning](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/e2023177a2fc2da3d14e0c799981c3525217a2bd/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter14-1%20Occam's%20Razor%20for%20Learning.png)
<center> 图一 Occam's Razor for Learning <sup>[1]</sup></center>
</br>


上图就是一个模型选择的例子，左边的模型很简单，可能有分错的情况；而右边的模型非常复杂，所有的训练样本都分类正确。但是，我们会选择左边的模型，它更简单，符合人类直觉的解释方式。
这样的结果带来两个问题：
- 什么模型称得上是简单的？
- 为什么简单模型比复杂模型要好？

因为机器学习的本质在于通过学习，寻找数据的规则，从而投入到实践中。要在一堆没有规律的数据中找到一个规律既能反映数据的本质规律，又能完全没有错误是非常非常困难的（甚至是不可能的），所以我们要做的并不是要100%完美的在训练数据中找到规律，让 $E_{in}$ 为0，而是应该找到数据本身符合的规律。加上噪音的无处不在，所以在合适的规律的模型中，出现一定的错误是很正常的。

所以在运用模型时，能使用简单的模型，就用简单的模型。简单而又实用的才是最好的！


</br></br>
----------------------------------

## 2. Sampling Bias
课上举了1948年美国选举，2个候选人A和B。一家报刊提前通过电话采访得到的结果是A会赢B，但是实际的结果却反过来。这是因为这家报刊在抽样调查的过程中只对某一阶层的人进行采访，所以导致了这个错误。

这个例子给我们的教训就是，如果抽样有偏差的话，那么学习的结果也产生了偏差，这种情形称之为抽样偏差Sampling Bias。从技术上来说，就是训练数据和验证数据要服从同一个分布，最好都是独立同分布的，这样训练得到的模型才能更好地具有代表性。


</br></br>
----------------------------------

## 3. Data Snooping
课上举的例子是偷窥数据造成对结果的影响。很显然，我们不应该提前去偷窥数据，但是这个实际上又是不可能避免的，因为我们在开始之前就会根据以往的经验选择性的偏向于某一类模型去处理问题，或者避免再用某一些模型去学习。

有2个方法可能尽可能减少数据偷窥
1. “看不见”数据。当我们在选择模型的时候，尽量用我们的经验和知识来做判断选择，而不是通过数据来选择。先选模型，再看数据。
2. 保持怀疑。不要对别人的说法信以为真，要通过自己的研究与测试来进行模型选择，这样得到比较正确的结论。



</br></br>
----------------------------------

## Summary
1. 首先介绍了Occam’s Razor - 越简单而有效的模型越好！
2. 然后说明了Sampling Bias的坏处 - 我们在训练时要保证数据的来源，最好是相互独立的
3. 最后说明了Data Snooping带来的坏处 - 尽量先选择模型，然后在去查看数据，然后在训练的过程要保持怀疑的态度


</br></br>
----------------------------------

## Reference
[1] 机器学习基石(台湾大学-林轩田)\16\16 - 1 - Occam-'s Razor (10-08)
